# Surprise
The less probability, the more surprise
> S(evento)=log2(p(evento))


# Entropy
The more surprise, the less entropy.

Entropy is the sum of surprise weighed by the probability.

https://plus.maths.org/content/information-surprise


Claude Shannon
Mathematical theory of information
https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf
